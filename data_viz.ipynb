{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from tickers_scraper import get_tickers\n",
    "import pickle\n",
    "from urllib import request, error\n",
    "import json\n",
    "from pandas_datareader import data as webdata\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Security:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "\n",
    "    def get_price(self, start, end):\n",
    "        \"\"\"\n",
    "        scrapes yahoo finance to get historical adjusted close price and volume\n",
    "        returns a dataframe\n",
    "        \"\"\"\n",
    "        ticker = self.ticker\n",
    "        # convert to timestamp\n",
    "        start_ts = int(pd.Timestamp(start).timestamp())\n",
    "        end_ts = int(pd.Timestamp(end).timestamp())\n",
    "\n",
    "        price_url = f'https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?symbol={ticker}&period1={start_ts}&period2={end_ts}&interval=1d&includePrePost=true&events=div%7Csplit%7Cearn&lang=en-US&region=US&crumb=qntR3hW5Tko&corsDomain=finance.yahoo.com'\n",
    "        with request.urlopen(price_url) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "\n",
    "        adj_close = data['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "        volume = data['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "        high = data['chart']['result'][0]['indicators']['quote'][0]['high']\n",
    "        low = data['chart']['result'][0]['indicators']['quote'][0]['low']\n",
    "        open_price = data['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "        timestamps = data['chart']['result'][0]['timestamp']\n",
    "        dates = [datetime.date.fromtimestamp(ts) for ts in timestamps] \n",
    "\n",
    "        df = pd.DataFrame({'Close': adj_close, 'Volume': volume,\n",
    "                          'Open': open_price, 'High': high,\n",
    "                          'Low': low}, index=dates)\n",
    "        df['Ticker'] = ticker\n",
    "\n",
    "        return df\n",
    "\n",
    "class ETF(Security):\n",
    "    def __init__(self, ticker):\n",
    "        super().__init__(self, ticker)\n",
    "    \n",
    "    \n",
    "class Company(Security):\n",
    "    def __init__(self, ticker):\n",
    "        super().__init__(self, ticker)\n",
    "\n",
    "    @property\n",
    "    def country(self):\n",
    "        return self.data['country']\n",
    "\n",
    "    @property\n",
    "    def sector(self):\n",
    "        return self.data['sector']\n",
    "\n",
    "    @property\n",
    "    def industry(self):\n",
    "        return self.data['industry']\n",
    "\n",
    "    @property\n",
    "    def city(self):\n",
    "        return self.data['city']\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.data['state']\n",
    "\n",
    "    @property\n",
    "    def zip_code(self):\n",
    "        return self.data['zip_code']\n",
    "\n",
    "    @property\n",
    "    def summary(self):\n",
    "        return self.data['summary']\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Scrapes Yahoo for latest fundamental data available\n",
    "\n",
    "        :return: a dictionary with the retrieved data\n",
    "        \"\"\"\n",
    "        fin_data_url = ('https://query2.finance.yahoo.com'\n",
    "                        f'/v10/finance/quoteSummary/{self.ticker}?formatted=true'\n",
    "                        '&lang=en-US'\n",
    "                        '&region=US'\n",
    "                        '&modules=summaryProfile'\n",
    "                        '%2CfinancialData'\n",
    "                        '%2CrecommendationTrend'\n",
    "                        '%2CupgradeDowngradeHistory'\n",
    "                        '%2Cearnings'\n",
    "                        '%2CdefaultKeyStatistics'\n",
    "                        '%2CcalendarEvents'\n",
    "                        '&corsDomain=finance.yahoo.com'\n",
    "                        )\n",
    "        with request.urlopen(fin_data_url) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "\n",
    "        try:\n",
    "            main_data = data['quoteSummary']['result'][0]['financialData']\n",
    "        except KeyError:\n",
    "            main_data = {}\n",
    "\n",
    "        try:\n",
    "            debt_to_equity = main_data['debtToEquity']['raw']\n",
    "        except KeyError:\n",
    "            debt_to_equity = np.nan\n",
    "        try:\n",
    "            earnings_growth = main_data['earningsGrowth']['raw']\n",
    "        except KeyError:\n",
    "            earnings_growth = np.nan\n",
    "        try:\n",
    "            profit_margin = main_data['profitMargins']['raw']\n",
    "        except KeyError:\n",
    "            profit_margin = np.nan\n",
    "        try:\n",
    "            roe = main_data['returnOnEquity']['raw']\n",
    "        except KeyError:\n",
    "            roe = np.nan\n",
    "        try:\n",
    "            rev_growth = main_data['revenueGrowth']['raw']\n",
    "        except KeyError:\n",
    "            rev_growth = np.nan\n",
    "        try:\n",
    "            n_shares = data['quoteSummary']['result'][0]['defaultKeyStatistics']['floatShares']['raw']\n",
    "        except KeyError:\n",
    "            n_shares = np.nan\n",
    "        try:\n",
    "            eps_t = data['quoteSummary']['result'][0]['defaultKeyStatistics']['trailingEps']['raw']\n",
    "        except KeyError:\n",
    "            eps_t = np.nan\n",
    "        try:\n",
    "            price_book = data['quoteSummary']['result'][0]['defaultKeyStatistics']['priceToBook']['raw']\n",
    "        except KeyError:\n",
    "            price_book = np.nan\n",
    "        try:\n",
    "            price = data['quoteSummary']['result'][0]['financialData']['currentPrice']['raw']\n",
    "        except KeyError:\n",
    "            price = np.nan\n",
    "        try:\n",
    "            eps_f = data['quoteSummary']['result'][0]['defaultKeyStatistics']['forwardEps']['raw']\n",
    "        except KeyError:\n",
    "            eps_f = np.nan\n",
    "        try:\n",
    "            beta = data['quoteSummary']['result'][0]['defaultKeyStatistics']['beta']['raw']\n",
    "        except KeyError:\n",
    "            beta = np.nan\n",
    "        try:\n",
    "            ev_ebitda = data['quoteSummary']['result'][0]['defaultKeyStatistics']['enterpriseToEbitda']['raw']\n",
    "        except KeyError:\n",
    "            ev_ebitda = np.nan\n",
    "        try:\n",
    "            ev_rev = data['quoteSummary']['result'][0]['defaultKeyStatistics']['enterpriseToRevenue']['raw']\n",
    "        except KeyError:\n",
    "            ev_rev = np.nan\n",
    "        try:\n",
    "            peg = data['quoteSummary']['result'][0]['defaultKeyStatistics']['pegRatio']['raw']\n",
    "        except KeyError:\n",
    "            peg = np.nan\n",
    "        try:\n",
    "            ebitda_margin = main_data['ebitdaMargins']['raw']\n",
    "        except KeyError:\n",
    "            ebitda_margin = np.nan\n",
    "        try:\n",
    "            n_analysts = main_data['numberOfAnalystOpinions']['raw']\n",
    "        except KeyError:\n",
    "            n_analysts = np.nan\n",
    "        try:\n",
    "            gross_margin = main_data['grossMargins']['raw']\n",
    "        except KeyError:\n",
    "            gross_margin = np.nan\n",
    "        try:\n",
    "            recommendation = main_data['recommendationKey']\n",
    "        except KeyError:\n",
    "            recommendation = np.nan\n",
    "        try:\n",
    "            target_price = main_data['targetMedianPrice']['raw']\n",
    "        except KeyError:\n",
    "            target_price = np.nan\n",
    "        try:\n",
    "            roa = main_data['returnOnAssets']['raw']\n",
    "        except KeyError:\n",
    "            roa = np.nan\n",
    "        try:\n",
    "            sector = data['quoteSummary']['result'][0]['summaryProfile']['sector']\n",
    "        except KeyError:\n",
    "            sector = np.nan\n",
    "        try:\n",
    "            industry = data['quoteSummary']['result'][0]['summaryProfile']['industry']\n",
    "        except KeyError:\n",
    "            industry = np.nan\n",
    "        try:\n",
    "            country = data['quoteSummary']['result'][0]['summaryProfile']['country']\n",
    "        except KeyError:\n",
    "            country = np.nan\n",
    "        try:\n",
    "            state = data['quoteSummary']['result'][0]['summaryProfile']['state']\n",
    "        except KeyError:\n",
    "            state = np.nan\n",
    "        try:\n",
    "            zip_code = data['quoteSummary']['result'][0]['summaryProfile']['zip']\n",
    "        except KeyError:\n",
    "            zip_code = np.nan\n",
    "        try:\n",
    "            city = data['quoteSummary']['result'][0]['summaryProfile']['city']\n",
    "        except KeyError:\n",
    "            city = np.nan\n",
    "        try:\n",
    "            address = data['quoteSummary']['result'][0]['summaryProfile']['address1']\n",
    "        except KeyError:\n",
    "            address = np.nan\n",
    "        try:\n",
    "            summary = data['quoteSummary']['result'][0]['summaryProfile']['longBusinessSummary']\n",
    "        except KeyError:\n",
    "            summary = np.nan\n",
    "\n",
    "        # Get this quarter's actual and estimated earnings, the previous quarter's actual and estimate and the\n",
    "        # last year's actual\n",
    "\n",
    "        ret_dict = {\n",
    "            'scrape_date': datetime.datetime.today().date(),\n",
    "            'debt_to_equity': debt_to_equity,\n",
    "            'earnings_growth': earnings_growth,\n",
    "            'profit_margin': profit_margin,\n",
    "            'roe': roe,\n",
    "            'rev_growth': rev_growth,\n",
    "            'eps_t': eps_t,\n",
    "            'price_book': price_book,\n",
    "            'ebitda_margin': ebitda_margin,\n",
    "            'n_analysts': n_analysts,\n",
    "            'gross_margin': gross_margin,\n",
    "            'eps_f': eps_f,\n",
    "            'price': price,\n",
    "            'recommendation': recommendation,\n",
    "            'target_price': target_price,\n",
    "            'roa': roa,\n",
    "            'summary': summary,\n",
    "            'country': country,\n",
    "            'city': city,\n",
    "            'state': state,\n",
    "            'zip_code': zip_code,\n",
    "            'sector': sector,\n",
    "            'industry': industry,\n",
    "            'beta': beta,\n",
    "            'ev_ebitda': ev_ebitda,\n",
    "            'ev_rev': ev_rev,\n",
    "            'peg': peg,\n",
    "            'n_shares': n_shares,\n",
    "            'address': address,\n",
    "        }\n",
    "        return ret_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tickers():\n",
    "    \"\"\"\n",
    "    Scrape wikipedia for tickers and names\n",
    "    :return: dictionary\n",
    "    \"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    with request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "    soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "    table = soup.find('table', attrs={'class': 'wikitable'})\n",
    "\n",
    "    companies = {}\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            ticker = cols[0].text.strip().lower()\n",
    "            name = cols[1].text.strip()\n",
    "            companies[ticker] = name\n",
    "    return companies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_company_data(names_and_tickers, pickle=None):\n",
    "    \"\"\"\n",
    "    Get ticker dictionary from wiki and build dataframe with financial data\n",
    "    :param debug: if true, use a single company (for testing)\n",
    "    :param pickle: if true, pickle result\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    tickers = list(names_and_tickers.keys())\n",
    "    companies = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        company = Company(ticker)\n",
    "        company_data = company.data.copy()\n",
    "        company_data['name'] = names_and_tickers[ticker]\n",
    "        df = pd.Series(company_data, name=ticker)\n",
    "        companies = pd.concat([companies, df], axis=1)\n",
    "\n",
    "    # transpose\n",
    "    companies = companies.transpose()\n",
    "\n",
    "    if pickle:\n",
    "        companies.to_pickle(path=pickle)\n",
    "    return companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_price_data(tickers, start, end, pickle=None):\n",
    "    \"\"\"\n",
    "    Build stacked dataframe with price time series (daily) for each ticker in given list\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    prices = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        company = Company(ticker)\n",
    "        try:\n",
    "            price_df = company.get_price(start, end)\n",
    "        except error.HTTPError:\n",
    "            print(f'No price data for {ticker}')\n",
    "            price_df = pd.DataFrame()\n",
    "        prices = pd.concat([prices, price_df], axis=0)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if pickle:\n",
    "        prices.to_pickle(path=pickle)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get tickers\n",
    "names_and_tickers = get_tickers()      \n",
    "tickers = list(names_and_tickers.keys())\n",
    "\n",
    "# replace brk.b for brk-b and bf.b for bf-b\n",
    "for old_key, new_key in {'brk.b': 'brk-b', 'bf.b': 'bf-b'}.items():\n",
    "    names_and_tickers[new_key] = names_and_tickers.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape data or read from pickle\n",
    "companies = build_company_data(names_and_tickers, pickle='sp500_data.pkl')\n",
    "# companies = pd.read_pickle(path='sp500_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No price data for brk.b\n",
      "No price data for bf.b\n"
     ]
    }
   ],
   "source": [
    "start = datetime.date(2008,1,1)\n",
    "end = datetime.datetime.today().date()\n",
    "\n",
    "prices = build_price_data(tickers, start, end, pickle='sp500_prices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To CSV\n",
    "companies.to_csv('sp500_data.csv')\n",
    "prices.to_csv('sp500_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berkshire Hathaway'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.loc['brk-b', 'name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
